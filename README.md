# Проект с интеграцией RAG.
В целях знакомства с RAG и практики в  
работе с промптами, придумал следущий проект, который в ближайшее время планирую реализовать:

### Идея проекта:
В модель загружаются фотографии комнаты. (поверхность стола, комода, подоконника, ..)  
Модель с помощью YOLO детектирует предметы из **заранее подготовленного конечного** множества классов.
(Для MVP. Потом можно добавить автоматическое расширение классов с помощью VLM)
В классах предметов заранее прописаны свойства, необходимые для определения и методы их определения.
Базовые методы определения полагаются более быстрыми,
но при низком confidence идет обращение к VLM для уточнения свойств или класса объекта.

Информация об объектах сохраняется в графовую БД с эмбеддингами (Neo4j 5.x).

Выполнение запроса я пока вижу следующим образом:
1. Подаем запрос в (пока) текстовом виде "скажи мне, где синяя штука для питья"
2. Он попадает на вход к LLM, которая искомый элемент конвертирует в эмбеддинг
3. В векторной БД находятся наиболее вероятные элементы, передаются в модель
4. После чего на вход LLM подается промпт с графовой базой данных, id найденных объектов и исходным запросом
5. На основании этих данных LLM генерирует ответ.

```mermaid
graph TD;
    A[Фото] --> B(YOLO-детекция)
    B --> C{Confidence > conf_threshold?}
    C -->|Да| D1[Стандартные признаки]
    C -->|Нет| D2[LLM → Какие признаки?]
    D1 & D2 --> E[Измерение признаков]
    E --> F[(Neo4j: Объекты + эмбеддинги)]
    G[Запрос] --> H[LLM: Структуризация]
    H --> I[Гибридный поиск в Neo4j]
    I --> J[Фильтр по признакам]
    J --> K{Один объект?}
    K -->|Да| L[Точный ответ]
    K -->|Нет| M[Берем первый + уточнение]
```


### P.S.
Выглядит так, будто я не могу развернуть VLM локально в силу ограниченных
вычислительных мощностей моего ноутбука. И у меня возникает идея развернуть API в Colab.  
Наверное пока это будет просто вывод в консоль с просьбой описать, что на картинке:)))
